---
title: "Practical Machine Learning Project"
author: "Oscar Villa"
date: "June 8, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis: 
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website on <http://groupware.les.inf.puc-rio.br/har>
The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set
## Getting, cleaning and tidying data

### 1. Loading training data previously downloaded from the respectives URLs: 
training <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>, and testing <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>
```{r gettin_data, cache=TRUE}
fileTraining <- "pml-training.csv"
fileTesting <- "pml-testing.csv"
```

- Import the data making empty values as NAs.
```{r reading_datasets, cache=TRUE}
training <- read.csv(fileTraining, na.strings=c("NA",""), header=TRUE)
testing <- read.csv(fileTesting, na.strings=c("NA",""), header=TRUE)
trainNames <- colnames(training)
testNames <- colnames(testing)
```
- Now check identity of the names except the last colnames; Have the two data sets the same columns? 
```{r validation_1}
all.equal(trainNames[-length(trainNames)], testNames[-length(testNames)])
```
### 2. Cleaning-tidying datasets:
- There are a lot of columns filled of NAs. Lets see how much
```{r check_NAs, cache=TRUE}
numNAsTraining <- apply(training, 2, function(x) sum(is.na(x)))
numNAsTesting <- apply(testing, 2, function(x) sum(is.na(x)))
saveRDS(numNAsTraining, "./numNAsTraining.RDS") 
saveRDS(numNAsTesting, "./numNAsTesting.RDS")
table(as.numeric(numNAsTraining))
```
- 60 of them have 0 NAs and 100 of them have 19216 NAs. That's mean all those 100 columns have a proportion of cases as NAs of `r max(as.numeric(numNAsTraining)) / nrow(training)`%

- Will not try to replace them because of the high proportion; instead will remove them because they doesn't allow to calculate predictions with the model if included as newdata.
```{r remove_NAsZeroCols, cache = TRUE, include=FALSE}
readRDS("./numNAsTraining.RDS")
readRDS("./numNAsTesting.RDS")
training <- training[,(numNAsTraining == 0)]
testing <- testing[,(numNAsTesting == 0)]
```
- Then let's check the number of NAs now
```{r numNAs, cache=TRUE}
sum(is.na(training))
sum(is.na(testing))
```
- I'll not take in account the first sevent columns, namely: `r colnames(training)[1:7]` because they talk about row names, user names, time stamps in differents formats and the windows of observations
```{r removing_first_seven_cols, cache=TRUE}
training <- training[, 8:ncol(training)]
testing <- testing[, 8:ncol(testing)]
```
- both data sets have the same columns except by the last one (class or output)?: are equals all the columns except the last one? 
```{r validation_n}
all.equal(names(training[,-ncol(training)]), names(testing[,-ncol(testing)]))
```
## 3. Training models:
- So far we have the test set ready-tidy to start applying machine learning. So let's partitioning the data like this: 60% for training (aka trainingAll), 20% for evaluating the models (aka testingTrained), and  20% for validation (aka validationtrained). Remember a testset for quiz prediction has holding out (aka testing). The trainingAll will then be splitted out on 4 folds (aka training1, training2, training3 and training4).
```{r spliting_data, cache=TRUE}
library(caret)
set.seed(1981)
inTrain <- createDataPartition(training$classe, p = 0.60, list = FALSE)
trainingAll <- training[inTrain, ]
testingAll <- training[-inTrain, ]
```
- Now gotta check that the partitioning did well done: it's the sum of the number of cases of the two new data equal to the number of cases of the original? `r nrow(trainingAll) + nrow(testingAll) == nrow(training)`

- Now we can proceed to split the testingAll dataset into two blocks, one for testing and the another for validation
```{r splitting_testingAll, cache=TRUE}
set.seed(1982)
inTrain <- createDataPartition(y = testingAll$classe, p = 0.50, list = FALSE)
testingTrained <- testingAll[inTrain, ]
validationtrained <- testingAll[-inTrain, ]
```
- Let's check if the new tow data sets partition conserves all the cases
```{r check_conservation_cases, cache=TRUE}
nrow(testingTrained) + nrow(validationtrained) == nrow(testingAll)
```

- It takes a lot of time to train a random forest model on a data set as big as this (rows = `r nrow(trainingAll)` x cols = `r ncol(trainingAll)`). I did try it for around 30 minutes with parallelizing for allowing the use of three of the cores of a Intel® Core™ i7-6500U CPU @ 2.50GHz × 4 with 8GB of RAM availables, but some issues appears with the closing of connections of the cluster for parallelization.
So, I tried rpart as method, and yields just 0.5073 of accuracy, not enough to pass the quiz. Tried lda as method, but yields just 0.7 of accuracy, not enough. Also svm with a accuracy of 0.89. It's not so bad, but random forest is better.

- I choose to split the dataset in 4 folds aiming to reduce the training random forest models time, and then stack them all four together with a random forest again; every random forest model with 3 cross validation.

- Splitting the data onto four folds:
```{r splitting_training, cache=TRUE}
set.seed(1983)
inTrain <- createFolds(y = trainingAll$classe, k = 4)
training1 <- trainingAll[inTrain$Fold1, ]
training2 <- trainingAll[inTrain$Fold2, ]
training3 <- trainingAll[inTrain$Fold3, ]
training4 <- trainingAll[inTrain$Fold4, ]
```
- Because I'm not confident with this partitioning method, have to ensure that there are not repeated cases (rows): make a vector with all the elements of the lists (the cases and find out if there are duplicates among them)
```{r validation3, cache=TRUE}
DF <- rbind(as.numeric(inTrain$Fold1, inTrain$Fold2, inTrain$Fold3, inTrain$Fold4))
duplicated(DF)
```
and then check no losses of rows
```{r validation4, cache=TRUE}
nrow(trainingAll) == nrow(training1) + nrow(training2) + nrow(training3) + nrow(training4)
```
- Now, following the instructions of Mentor Len Greski on <https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md>, my objective it's to speed up the process:

-- First set up training set to the x / y syntax because model format performs poorly on the formula way
```{r formatting_dataset, cache=TRUE}
x1 <- training1[, -ncol(training1)]
y1 <- training1[, ncol(training1)]
x2 <- training2[, -ncol(training2)]
y2 <- training2[, ncol(training2)]
x3 <- training3[, -ncol(training3)]
y3 <- training3[, ncol(training3)]
x4 <- training4[, -ncol(training4)]
y4 <- training4[, ncol(training4)]
saveRDS(x1, "./x1.RDS")
saveRDS(y1, "./y1.RDS")
saveRDS(x2, "./x2.RDS")
saveRDS(y2, "./y2.RDS")
saveRDS(x3, "./x3.RDS")
saveRDS(y3, "./y3.RDS")
saveRDS(x4, "./x4.RDS")
saveRDS(y4, "./y4.RDS")
```
-- Configure parallel processing
```{r config_cluster, cache=TRUE}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
```
-- Configure trainControl object
```{r config_trainControl_object, cache=TRUE}
fitControl <- trainControl(method = "cv",
                           number = 3,
                           allowParallel = TRUE)
```
-- Then do train the models
```{r train_models, include=FALSE, cache=TRUE}
readRDS("./x1.RDS")
readRDS("./y1.RDS")
readRDS("./x2.RDS")
readRDS("./y2.RDS")
readRDS("./x3.RDS")
readRDS("./y3.RDS")
readRDS("./x4.RDS")
readRDS("./y4.RDS")
set.seed(1984)
mdl1 <- train(x1, y1, method = "rf", trControl = fitControl, data = training1)
mdl2 <- train(x2, y2, method = "rf", trControl = fitControl, data = training2)
mdl3 <- train(x3, y3, method = "rf", trControl = fitControl, data = training3)
mdl4 <- train(x4, y4, method = "rf", trControl = fitControl, data = training4)
```
-- Finally de-register parallel processing cluster
```{r stopCluster, cache=TRUE}
stopCluster(cluster)
```
-- Now we'll make predictions for each individual model and prepare metrics with the confusionMatrix for comparison with the stacked models:
```{r metrics1, cache=TRUE}
pred1 <- predict(mdl1, testingTrained)
pred2 <- predict(mdl2, testingTrained)
pred3 <- predict(mdl3, testingTrained)
pred4 <- predict(mdl4, testingTrained)
sens <- rbind(mdl1 = t(confusionMatrix(pred1, testingTrained$classe)$byClass)[1,], 
                  mdl2 = t(confusionMatrix(pred2, testingTrained$classe)$byClass)[1,], 
                  mdl3 = t(confusionMatrix(pred3, testingTrained$classe)$byClass)[1,], 
                  mdl4 = t(confusionMatrix(pred4, testingTrained$classe)$byClass)[1,])
spec <- rbind(mdl1 = t(confusionMatrix(pred1, testingTrained$classe)$byClass)[2,], 
              mdl2 = t(confusionMatrix(pred2, testingTrained$classe)$byClass)[2,], 
              mdl3 = t(confusionMatrix(pred3, testingTrained$classe)$byClass)[2,], 
              mdl4 = t(confusionMatrix(pred4, testingTrained$classe)$byClass)[2,])
sensAvg <- t(data.frame(allMdlsAvg = colMeans(sens)))
specAvg <- t(data.frame(allModelsAvg = colMeans(spec)))
```
- Stacking models
-- First prepare the new data frame
```{r mystack, cache=TRUE}
myStack <- data.frame(mdl1 = pred1, 
                      mdl2 = pred2, 
                      mdl3 = pred3, 
                      mdl4 = pred4, 
                      response = testingTrained$classe)
```
-- Parallelizing again (not necessary to make it again step by step):
```{r parallelizing_trainning_stack_model, cache=TRUE}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv",
                           number = 3,
                           allowParallel = TRUE)
x <- myStack[, -ncol(myStack)]
y <- myStack[, ncol(myStack)]
mdl <- train(x, y, method = "rf", trControl = fitControl, data = myStack)
stopCluster(cluster)
```
- Predict for measuring improvements obtained by stacking the models, so that 

-- The average sensitivity of the four models are:
```{r sensitivity}
sensAvg
```
-- The average specificity of the four models are:
```{r accuracies}
specAvg
```
-- The sensitivities of the stacked models are:
```{r metrics3, cache=TRUE}
pred <- predict(mdl, myStack)
ppalMetrics <- t(confusionMatrix(pred, myStack$response)$byClass)[1:2,]
ppalMetrics[1,]
```

-- and specificities:
```{r metrics4, cache=TRUE}
pred <- predict(mdl, myStack)
ppalMetrics <- t(confusionMatrix(pred, myStack$response)$byClass)[1:2,]
ppalMetrics[2,]
```

-- We can see that the gain on sensitivity obtained by stacking the models are:
```{r gain}
(ppalMetrics[1, ] - sensAvg)[,1:5]
```
so that stacking the models just increase the sensitvity average by `r mean((ppalMetrics[1, ] - sensAvg)[,1:5])`

-- and specificity gained by stacking the models are:
```{r gain2}
(ppalMetrics[2, ] - specAvg)[,1:5]
```
on average, the increase is just of `r mean((ppalMetrics[2, ] - sensAvg)[,1:5])`

## 4. Selecting final model
As we have seen, here a lot is about a trade off of performance: speed vs sensitivity-specificity (accuracy).
Stacking the models doesn't really increases the accuracy and sensitivity significantly, but do takes a lot of time to train the final model. Instead, if you train the model with just on of the test set, it will yield almost the same sensitivity specificity (accuracy too) and takes some like 1/5 of the time. In this case, the splitting on four folds-train random forest models-and measure accuracy will keep just a as an additional cross validation with four folds.
So, I choose to: 

- take the model trained on just one of the four folds; the one with the best performance
```{r, cache=TRUE}
rowMeans(sens)
rowMeans(spec)
```

namely, model2 (mdl2)
- predict on the testingTrained for obtain metrics
```{r metrics5, cache=TRUE}
confusionMatrix(pred2, testingTrained$classe)
```

- predict on validationtrained and obtain the metrics
```{r metrics6, cache=TRUE}
predFinal <- predict(mdl2, validationtrained)
confusionMatrix(predFinal, validationtrained$classe)
```

As expected, the performance is a less than the optimistic one on testingTrained set
- finally predict on the testing (dataset for quiz) to obtain the quiz responses
```{r solveQuizz, cache=TRUE}
predict(mdl2, testing)
```

 I got 18/20 = `r 18/20` on quiz, 0.06 less than the accuracy expect when the model was measured.